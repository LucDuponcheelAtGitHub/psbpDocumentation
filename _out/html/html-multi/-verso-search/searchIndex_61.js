window.docContents[61].resolve({"/Programs-versus-Computations/#PSBP-Documentation--Programs-versus-Computations":{"id":"/Programs-versus-Computations/#PSBP-Documentation--Programs-versus-Computations","header":"3. Programs versus Computations","context":"PSBP Documentation","contents":"The standard Lean library already enables Computation Specification Based Programming.\n\nSo why promoting Program Specification Based Programming in the first place?\n\nIn short, informally, it is all a matter of taste\n(you may wish to give up reading this document).\n\nIn short, formally, because of progressive insight into what programming, writing code, is all about.\n\nLet's explain this in terms of the history about my progressive insight\n(you may wish to ignore what follows, not being interested in my personal history).\n\nI am a retired mathematician.\n\nMathematics is generally agreed upon to be useful to understand the reality we are all part of. For example to\nunderstand problem domains some of us are, professionaly, confronted with.\n\nBridge building engineers may benefit form studying appropriate mathematics to understand what bridges are all about.\nVehicle building engineers may benefit form studying appropriate mathematics to understand what vehicles are all about.\nLikewise, programming engineers (programmers) may benefit form studying appropriate mathematics to understand what\nprograms are all about.\n\nSeparating specifications from implementations is generally agreed upon to be useful to understand problem domains.\n\nA bridge specification states, for example, that it must be able to carry the weight of a number of vehicles. How it is\nable to carry that weight is an implementation concern. Maybe one bridge is more pleasing to look at as another one. A\nvehicle specification states, for example, that it must be able to transport a number of passengers. How it is able to\ntransport that number of passengers is an implementation concern. Maybe one car is more comfortable than another one. A\nprogram specification states, for example, that a program must be able to create composite data and to perform\nconditional logic. How it is able to create composite data and to perform conditional logic is an implementation\nconcern. Maybe one program is more CPU (or GPU or NPU) effecient, or less RAM consuming than another one.\n\nNote that I wrote bridge, resp. car, resp. program as an abbreviation of materialization corresponding to an implementation\nof the bridge specification, resp. materialization corresponding to an implementation of the car specification, resp.\nmaterialization corresponding to an implementation of the program specification.\n\nI have always been interested in mathematics, so I followed the typical mathematics oriented education path from\nsecondary school all the way to obtaining a PhD in mathematics. Now I realize that obtaining a PhD (in mathematics) is\njust a beginning. It shows that you are able to use 20% imagination and 80% transpiration to be creative in a particular\narea (of mathematics).\n\nI did, among others, mathematics research on\nNon-archimedean induced representations of compact zerodimensional groups.\nNot that it matters much for the content of this document. What does matter is that I soon realized that, those days, in\nBelgium, mathematics could mainly be done as a backyard ritual, so, in order to earn money for a living, I decided to\nbecome a programmer. being adicted to mathematics research, I also decided to do computer science research as a late at\nnight hobby (typically starting at 9PM).\n\nI studied function level programming systems, supported by the pointfree, effectfree functional programming language\nFP.\n\nI published a paper\nAcceptable functional programming systems\nabout a pointfree effectfree functional programming library. I wrote the paper together with my twin brother Marc, who\nis also a mathematician. The library was written using the\nPascal programming language.\n\nThe first functional programming language I used extensively was\nMiranda.\nMiranda turned out to be a perfect language to learn (not to say become addicted to) functional programming.\n\nThe next functional programming language I used extensively was Gofer, later evolving to\nHugs.\nGofer was the first functional programming language supporting type constructor classes. They are appropriate to write\nthe computing related specifications of effectful pointful functional programming libraries. The mathematical\nfoundations of those specifications are monads.\n\nI published the following papers\n\n1. Composing monads,2. On the expressive power of constructor classes,3. Deterministic crror-correcting combinator parsers, and4. Using catamorphisms, subtypes and monad transformers for writing modular functional interpeters.\n\nThe first paper was written, as a late night hobby, together with Mark P. Jones, the author of Gofer. What is\nspecial about that paper is that we had never physically met each other. Those were the early days of the internet. We\nwrote the paper (and the code that came with it) together by sending emails with attachments to each other. In fact, it\nturned out to be an efficient way to work together. While one of us was sleeping (in Europe resp. the USA) the other one\nwas working (in the USA resp. Europe). The paper also contained some equational proofs. Those days, they were simply\nencoded as lists of expressions representing equational proof steps. Gofer did not have a formal proof correctness\nreviewing mechanism like Lean has, but, type correctness provided at least some formal proof correctness confidence.\n\nThe other papers were written when I was working, for two years, at the University of Utrecht. That was a unique\nexperience for me for which I am, forever, greatful to Doaitse Swierstrsa. Erik Meijer, also working at the University\nof Utrecht, invited me for a lecture, as such introducing me to Doaitse. Apart from being an outstanding computer\nscientist, Doaitse was the best people manager I have ever worked for. Erik does not need any introduction. He became\na living legend. Also Graham Hutton worked at the University of Utrecht when I was working there. I was in good company.\n\nThe second paper was written together with Erik Meijer and was presented at a conference organized by the Glasgow\nUniversity, the center of Functional Programming in the UK (Philip Wadler and Simon Peyton Jones worked there).\n\nThe third paper was written together with Doaitse Swierstra. It turned out to be a motivation for two type\nconstructor classes,\narrows and\napplicatives,\nbased upon mathematical foundations: Arrow based libraries are pointfree effectful functional programming libraries.\nApplicative based libraries are pointful effectful functional programming libraries. The relationship between them was\nexplored in\nArrows and applicatives\n\nThe fourth paper was based on\nMonad transformers and modular interpreters.\nI added catamorphisms and subtypes to make interpreter alternatives reusable. The paper has never been published.\nAnyway, somehow, surprisingly, the paper turned out to be available online and it has been cited several times.\n\nAll this brings me to the progressive insight that motivates me to do this Lean PSBP project.\n\nWhat are the most appropriate mathematical foundations and corresponding type classes for effectful functional\nprogramming? The more powerful they are the less implementation flexibility they have. For example monadic parsers\ncan parse context sensitive grammars, while applicative parsers cannot parse context sensitive grammars, but,\napplicative parsers allow for more flexible error handling than monadic parsers.\n\nProgramming is also about elegance and programming libraries are also about ease of use. I.m.h.o. pointfree programming\nis more elegant than pointful programming and pointfree programming libraries are easier of use than pointful\nprogramming libraries.\n\nOf course this is a matter of taste, but let me motivate my taste ... .\n\nThe Applicative specification and the Monad specification specify computation capabilities. Think of computations as\neffectful expressions. They are operational artifacts. They do not really have a meaning in the mathematical sense and\ncannot be given a meaningful name. How, for example, would you name expression x * x in λ x => x * x ?\nJust like expressions are evaluated to yield a value, computations, by somehow executing them, yield a value, but,\nsomehow executing computations may also perform side effects along the way. The \"somehow\" in the previous sentence is\nimportant, because it depends on the materialization corresponding to instances of the type constructor classes in\nterms of whose members the computations (recall, more precisely, computation specificatons) have been written.\n\nThe program specification of this document is similar to, but more powerful than, the arrow specification. Think of\nprograms as arrows with choice.\n\nThe program specification of this document specifies, not surprisingly, program capabilities. Think of programs as\neffectful functions. They are denotational artifacts. They do have a meaning in the mathematical sense and can be given\na meaningful name. For example λ x => x * x can be given a the meaningful name square. Of course functions and\nprograms can also be looked at as operational artifacts. Just like functions, programs, by somehow running\nthem, transform an initial value to a final value, but, somehow running them may perform side effects along the way. The\n\"somehow\" in the previous sentence is important, because it depends on the materialization corresponding to instances of\nthe type constructor classes in terms of whose members the programs (recall, more precisely, program specificaton) have\nbeen written.\n\nBy the way, a value can be a basic-value or a composite-value, repesented as a (nested) tuple. As such\nvalues are also components of a component system.\n\nWhy going for pointfree programs instead of pointful applicatives or pointful monads?\nI.m.h.o. it is more natural to think denotationally, about \"what\", than to think operationally, about \"how\".\n\nLet's try to illustrate this with some Lean code.\n\nThe computing related type class Bind has a member\n\nbind :\n  {α β : Type} →\n  computation α → (α → computation β) → computation β\n\n\nwith an associativity law (>>= is infix notation for bind)\n\nbind_assoc\n  (cα : computation α)\n  (αfcβ : α → computation β)\n  (βfcγ : β → computation γ) :\n    cα >>= αfcβ >>= βfcγ = cα >>=\n      λ α => αfcβ α >>= βfcγ\n\n\nThe programming related type class Sequential has a member\n\nandThen\n  {α β γ : Type} :\n  program α β → program β γ → program α γ\n\n\nwith an associativity law (>=> is infix notation for andThen)\n\nandThen_assoc\n  (αpβ : program α β)\n  (βpγ : program β γ)\n  (γpδ : program γ δ) :\n  (αpβ >=> βpγ) >=> γpδ =\n    αpβ >=> (βpγ >=> γpδ)\n\n\nLet's first consider syntax.\n\nI can more easily remember the definition of andThen_assoc than the definition of bind_assoc.\n\nWhat about you?\n\nLet's next consider semantics.\n\nI can more easily explain andThen and andThen_assoc than bind and bind_assoc. I'll give it a try.\n\nWhat about you? Give it a try.\n\nLet's first deal with Sequential.\n\nI think of a function as transforming an initial value yielding a final value (transforming an argument to a result).\n\nLikewise, I think of a program as transforming an initial value yielding a final value (transforming an argument to a\nresult (potentially performing side effects along the way)).\n\nLet's ignore side effects for now (you may wish to explain what happens with them).\n\nandThen can be explained as:\n\ntransforming an initial value of type α, using a program of type program α β, yielding an intermediate value of type\nβ, and then transforming that intermediate value, using a program of type program β γ, yielding a final value of\ntype γ.\n\nandThen_assoc can be explained as:\n\nfirst transforming an initial value of type α, using αpβ >=> βpγ, yielding an intermediate value of type γ, and\nthen transforming that intermediate value, using γpδ, yields the same final value as first transforming the initia\nvalue of type α, using αpβ, to an intermediate value of type β, and then transforming that intermediate value,\nusing βpγ >=> γpδ.\n\nLet's second deal with Bind.\n\nI think of a evaluating an expression as yielding a value.\n\nLikewise, I think of a executing a computation as yielding a value (potentially performing side effects along the way).\n\nLet's ignore side effects for now  (you may wish to explain what happens with them).\n\nbind can be explained as:\n\nexecuting an inner computation of type computation α yielding an intermediate value of type α, and then binding that\nintermediate value to an outer computation valued function of type α → computation β, yields an outer computation of\ntype computation β (that when executing it yields a value of type β)\n\nor\n\nexecuting an inner computation of type computation α yielding an intermediate value of type α, and then transforming\nthat intermediate value using an outer computation valued function of type α → computation β, yields an outer\ncomputation value of type computation β (that when executing it yields a value of type β)\n\nbind_assoc can be explained as:\n\nexecuting an inner computation of cα yielding a first intermediate value, and then transforming that intermediate\nvalue using an intermediate computation valued function αfcβ, yielding an intermediate computation value that, when\nexecuting it, yields a second intermediate value, and then transforming that intermediate value using a final\ncomputation valued function βfcγ yields the same final outer computation value as executing the inner computation of\ncα yielding a first intermediate value, and then transforming that intermediate value using the computation valued\nfunction that first transforms that first intermediate value using the intermediate computation valued function αfcβ,\nyielding an intermediate computation value that, when executing it, yields a second intermediate value, and then\ntransforming that intermediate value using a final computation valued function βfcγ.\n\nAs far as components of a component system is concerned, I also like programs more than computations.\n\nLet's have a closer look at the associativity law code fragments\n\n(αpβ >=> βpγ) >=> γpδ =\n  αpβ >=> (βpγ >=> γpδ)\n\n\nand\n\ncα >>= αfcβ >>= βfcγ =\n  cα >>= λ α => αfcβ α >>= βfcγ\n\n\nPrograms are closed components, while computations are open components. Computation cα needs to be opened to access\nthe value α yielded by executing it, so that it can be transformed using computation valued function αfcβ.\nProgramming with computations is pointful programming.\n\nPrograms do not need to be opened. Programming with programs is pointfree programming. Think of using them as playing\nwith Lego ariifacts.\n\nI.m.h.o, it is a more elegant and easier to program pointfree than to program pointful. Likewise, i.m.h.o., it is more\nelegant and easier to reason in terms of pointfree laws than to reason in terms of pointful laws. Of course, this is a\nmatter of taste. Nevertheless hope to convince you that pointfree is more better than pointful.\n\nIt is possible, and sometimes more elegant, use programs positionally. Positional programming is similar to pointful\nprogramming. It is is useful for writing sequential recipe-like programs, where, starting from an initial value\n(often a composite-value), intermediate values (mostly basic-values) are created, and, together with the initial value,\nas a compositional value are passed to the next step of the recipe-like program, until a final value is yielded. The\ninitial value and intermediate values are accessed positionally. The creation of the intermediate values can involve\ngeneral programs. A sequential recipe-like program glues programs together, similar to an operating system scripting\nlanguage gluing operating system executables together.\n\n"}});